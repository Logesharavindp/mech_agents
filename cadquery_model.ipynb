{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running data preprocessing...\n",
      "Loading raw dataset...\n",
      "Train samples: 10000, Test samples: 5000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee03e6523070494c827b8ba05f348fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 1:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11389f5492d44de3b21b21b148fae369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2839bafcc9d461996b726f0a6e37da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 2:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5739a71d39c94374813c2584f7129b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e28e48b4bbd4a9c8ddf9b8bcc9fe24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 3:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfa9d4b6ca54a7c8255d693ecfdbbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4042d33bbbfb45f8ba7b25962855ea6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 4:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4130fc12e346b6b0891359ba93083b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4804f4b4284195ba85977a1c8eceff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 5:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3253d5bde84a6992fc4d250b3ba827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1f59f7f12c4e91835b521a798bfa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 6:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb97bffe43f400f8d2451ade5fab181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf9b4ff05d94f5085b932e1c178cdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 7:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a035583528ba419781df7d1f7c05048a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93189b083b794d538647bd7840ecac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 8:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e656924b7d2f4bdcbe8c1bbce160f092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2aad5dea7144ad949f0bc3eae92563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 9:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500274395bff44929aef74052fce9c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a01446b4b84d0e80e5f166e2353c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing train shard 10:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92280d956d57433da6328e8c9e6a5e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670bf74ac0b1447c86398dbf041a938e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 1:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4edae80b8042a69f24f0c4ad83dec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955122f2d4f3491f8d7ed6401689151c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 2:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f60789cd0be4cf387df2c93fa1da64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32d38a08fae4709a4870df6e4c491d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 3:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8620af962e794f3ba79a0d9edc5d391a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf5785c275e4d0289f45096fd2f74a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 4:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908b9b1f055f4540ae6f3762684f0583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4285f24ad75046f08bdd36cae29c66a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 5:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7416842961874b08810a169d3189a824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03415ab03ce54f0cb8cdf75cf766153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 6:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0985a3bb4e7d4e5a9dfe1488176f16b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a972d7980b804b819a585d8453f8114e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 7:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc977c5d1b364898a2b7b26ce0926268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527da8a2adf34aa7a5a58d32817fbf77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 8:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec36650d61546598598b0bf2e8a93a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdc495502094a1bbd9458b7e993eab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 9:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa96bc852ff4437ade3b7b5f29a3c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7efeea42f24e2e809d07414f382565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing test shard 10:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1dab54326042c284ab36868ae36711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete!\n",
      "✅ Loaded - Train: 10000, Test: 5000\n",
      "Sample: dict_keys(['pixel_values', 'labels', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: DATA PREPROCESSING AND LOADING\n",
    "# -------------------------------------------------\n",
    "# This cell performs several preprocessing steps:\n",
    "# 1. Loads the raw CAD dataset from Hugging Face.\n",
    "# 2. Shuffles and subsets the training and test splits.\n",
    "# 3. Preprocesses each batch by resizing images and tokenizing text.\n",
    "# 4. Saves the preprocessed data as multiple shards to disk.\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoImageProcessor\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Config parameters for data processing\n",
    "CACHE_DIR = \"/Volumes/BIG-DATA/HUGGINGFACE_CACHE\"\n",
    "OUTPUT_DIR = \"/Volumes/BIG-DATA/processed_gencad\"\n",
    "NUM_SHARDS = 10\n",
    "BATCH_SIZE = 8\n",
    "MAX_SAMPLES = 10_000\n",
    "\n",
    "# Setup processors with pre-trained models\n",
    "encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "decoder_checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n",
    "image_processor = AutoImageProcessor.from_pretrained(encoder_checkpoint)\n",
    "\n",
    "# Ensure the GPT2 tokenizer has a PAD token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_data():\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    print(\"Loading raw dataset...\")\n",
    "    ds = load_dataset(\"CADCODER/GenCAD-Code\", num_proc=4, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    # Shuffle and subset the dataset for training and testing\n",
    "    ds[\"train\"] = ds[\"train\"].shuffle(seed=42).select(range(min(MAX_SAMPLES, len(ds[\"train\"]))))\n",
    "    ds[\"test\"] = ds[\"test\"].shuffle(seed=42).select(range(min(5000, len(ds[\"test\"]))))\n",
    "    \n",
    "    print(f\"Train samples: {len(ds['train'])}, Test samples: {len(ds['test'])}\")\n",
    "    \n",
    "    def preprocess_batch(batch):\n",
    "        # Preprocess images to 224x224 and convert to FP16 format\n",
    "        pixel_values = image_processor(\n",
    "            batch[\"image\"],\n",
    "            return_tensors=\"pt\",\n",
    "            size={\"height\": 224, \"width\": 224}\n",
    "        ).pixel_values.to(dtype=torch.float16)\n",
    "        \n",
    "        # Preprocess text by tokenizing with max_length and padding\n",
    "        text_inputs = tokenizer(\n",
    "            batch[\"cadquery\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": [pv.numpy() for pv in pixel_values],\n",
    "            \"labels\": [lb.numpy() for lb in text_inputs.input_ids],\n",
    "            \"attention_mask\": [am.numpy() for am in text_inputs.attention_mask]\n",
    "        }\n",
    "    \n",
    "    # Process and save dataset in shards to avoid huge files\n",
    "    for split_name in [\"train\", \"test\"]:\n",
    "        split = ds[split_name]\n",
    "        for i in range(NUM_SHARDS):\n",
    "            shard = split.shard(num_shards=NUM_SHARDS, index=i)\n",
    "            shard = shard.map(\n",
    "                preprocess_batch,\n",
    "                batched=True,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                remove_columns=split.column_names,\n",
    "                desc=f\"Processing {split_name} shard {i+1}\"\n",
    "            )\n",
    "            shard.set_format(type=\"torch\", columns=[\"pixel_values\", \"labels\", \"attention_mask\"])\n",
    "            shard.save_to_disk(f\"{OUTPUT_DIR}/{split_name}_shard{i}\")\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\" Preprocessing complete!\")\n",
    "\n",
    "# Function to load all preprocessed shards and concatenate them\n",
    "def load_processed_data():\n",
    "    train_shards = [load_from_disk(f\"{OUTPUT_DIR}/train_shard{i}\") for i in range(NUM_SHARDS)]\n",
    "    train_ds = concatenate_datasets(train_shards)\n",
    "    \n",
    "    test_shards = [load_from_disk(f\"{OUTPUT_DIR}/test_shard{i}\") for i in range(NUM_SHARDS)]\n",
    "    test_ds = concatenate_datasets(test_shards)\n",
    "    \n",
    "    return train_ds, test_ds\n",
    "\n",
    "# Run preprocessing if the output directory does not exist\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    print(\"Running data preprocessing...\")\n",
    "    preprocess_data()\n",
    "\n",
    "# Load the preprocessed datasets and print a summary\n",
    "train_ds, test_ds = load_processed_data()\n",
    "print(f\" Loaded - Train: {len(train_ds)}, Test: {len(test_ds)}\")\n",
    "print(\"Sample:\", train_ds[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca65bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VisionEncoderDecoder model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model setup complete!\n",
      "Encoder: ViTModel\n",
      "Decoder: GPT2LMHeadModel\n",
      "Vocab size: 50257\n",
      "Model parameters: 239,195,904\n",
      "Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: MODEL SETUP AND CONFIGURATION\n",
    "# -------------------------------------------------\n",
    "# This cell sets up the VisionEncoderDecoder model.\n",
    "# Steps include:\n",
    "# 1. Loading the encoder (ViT) and decoder (GPT2) from pre-trained checkpoints.\n",
    "# 2. Loading the corresponding tokenizer and image processor.\n",
    "# 3. Configuring the tokenizer to include a padding token if missing.\n",
    "# 4. Adjusting the model configuration parameters for token IDs and generation.\n",
    "# 5. Defining a simple data collator for batching during training.\n",
    "# 6. Moving the model to GPU if available.\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Define model checkpoints for encoder and decoder\n",
    "encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "decoder_checkpoint = \"gpt2\"\n",
    "\n",
    "# Load the VisionEncoderDecoder model from the specified checkpoints\n",
    "print(\"Loading VisionEncoderDecoder model...\")\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_checkpoint,\n",
    "    decoder_checkpoint\n",
    ")\n",
    "\n",
    "# Load tokenizer and image processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n",
    "image_processor = AutoImageProcessor.from_pretrained(encoder_checkpoint)\n",
    "\n",
    "# Ensure the GPT2 tokenizer has a pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure the model's token IDs and vocabulary size\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id else tokenizer.eos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# Set additional generation parameters\n",
    "model.config.max_length = 256\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "\n",
    "# Define a data collator to prepare batches with proper padding for training\n",
    "def data_collator(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    labels = torch.stack([example[\"labels\"] for example in batch])\n",
    "    \n",
    "    # Replace padding token ids with -100 for loss computation\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "print(\" Model setup complete!\")\n",
    "print(f\"Encoder: {model.encoder.__class__.__name__}\")\n",
    "print(f\"Decoder: {model.decoder.__class__.__name__}\")\n",
    "print(f\"Vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f718f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3: MODEL TRAINING AND EVALUATION\n",
    "# -------------------------------------------------\n",
    "# This cell trains the model and evaluates its performance.\n",
    "# Steps include:\n",
    "# 1. Defining the training arguments (hyperparameters, logging, saving strategies).\n",
    "# 2. Initializing the Seq2SeqTrainer with the training and evaluation datasets.\n",
    "# 3. Training the model with error handling and saving checkpoints.\n",
    "# 4. Running a simple inference test post training.\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Set up training hyperparameters and arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./baseline_model\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=4,  # Low batch size to conserve memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,  # Effective batch size becomes 8\n",
    "    num_train_epochs=1,  # Use one epoch for initial testing\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    logging_steps=50,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    dataloader_num_workers=0,  # Prevent multithreading issues\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=True,\n",
    "    fp16=False,  # Disable FP16 to avoid precision errors\n",
    "    report_to=None,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "print(\"Training arguments:\")\n",
    "print(f\"- Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"- Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"- Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"- Output dir: {training_args.output_dir}\")\n",
    "\n",
    "# Initialize the trainer with the model, training arguments, and datasets\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds.select(range(100)),  # Use a smaller evaluation set to avoid memory issues\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Eval samples: 100\")\n",
    "\n",
    "# Train the model with error handling\n",
    "try:\n",
    "    # Clear CUDA cache before training if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    trainer.train()\n",
    "    print(\"✅ Training completed successfully!\")\n",
    "    \n",
    "    # Save the final trained model and tokenizer\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    \n",
    "    print(f\"💾 Model saved to {training_args.output_dir}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Training failed: {e}\")\n",
    "    print(\"Trying to save current state...\")\n",
    "    try:\n",
    "        trainer.save_model(\"./emergency_save\")\n",
    "        print(\"Emergency save completed\")\n",
    "    except:\n",
    "        print(\"Emergency save also failed\")\n",
    "\n",
    "# Simple inference test to validate model generation\n",
    "print(\"\\n🔍 Testing inference...\")\n",
    "try:\n",
    "    sample = test_ds[0]\n",
    "    pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(model.device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            pixel_values=pixel_values,\n",
    "            max_length=64,\n",
    "            num_beams=2,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(f\"Generated: {generated_text[:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Inference failed: {e}\")\n",
    "\n",
    "print(\"✅ Script completed!\")\n",
    "\n",
    "# Memory cleanup to free GPU and system memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1299345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Test samples: 500\n",
      "\n",
      "🔍 Running inference...\n",
      "\n",
      "=== Predicted CadQuery Code ===\n",
      " cad as a cad\n",
      " Gener aplane sketch for 0wpsch =qWork(qPl(qVector0000125 0006,.,.),qVector100600000001 00 00006;qVector612399367e17 -.,006 -.2335766644-).To006 00074742263,.To00742263474253line1line006line106line007line001line003line000line06line0100101line01line06ext0007ext0\n",
      "\n",
      "=== Ground Truth CadQuery Code ===\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(0.0, 0.0, -0.078125), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop0=wp_sketch0.moveTo(0.039473684210526314, 0.0).lineTo(0.039473684210526314, 0.05921052631578947).lineTo(0.29605263157894735, 0.05921052631578947).lineTo(0.29605263157894735, 0.0).lineTo(0.3289473684210526, 0.0).lineTo(0.3289473684210526, 0.05921052631578947\n"
     ]
    }
   ],
   "source": [
    "# Inference Cell: Generate and compare CadQuery code\n",
    "# -------------------------------------------------\n",
    "# Steps:\n",
    "# 1. Load the trained model, tokenizer, and image processor from a checkpoint.\n",
    "# 2. Load a preprocessed test shard.\n",
    "# 3. Select a sample from the dataset.\n",
    "# 4. Generate predicted CadQuery code using the model.\n",
    "# 5. Print both the predicted and ground truth CadQuery code for comparison.\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoImageProcessor\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Specify paths for checkpoint and processed test data\n",
    "CHECKPOINT_DIR = \"baseline_model_checkpoint-20250830T093207Z-1-001/baseline_model_checkpoint\"\n",
    "PROCESSED_DATA_DIR = \"/Volumes/BIG-DATA/processed_gencad\"  # adjust if needed\n",
    "\n",
    "print(\"Loading trained model...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(CHECKPOINT_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Move model to GPU if available and set to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "print(\"Loading test dataset...\")\n",
    "test_ds = load_from_disk(f\"{PROCESSED_DATA_DIR}/test_shard0\")\n",
    "print(f\"Test samples: {len(test_ds)}\")\n",
    "\n",
    "# Pick one sample and prepare its pixel values for inference\n",
    "sample = test_ds[0]\n",
    "pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "\n",
    "print(\"\\n Running inference...\")\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values=pixel_values,\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the predicted and ground truth CadQuery code\n",
    "print(\"\\n=== Predicted CadQuery Code ===\")\n",
    "print(generated_text)\n",
    "\n",
    "print(\"\\n=== Ground Truth CadQuery Code ===\")\n",
    "print(tokenizer.decode(sample[\"labels\"], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd70eebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n",
      "Test samples: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predicted CadQuery Code ===\n",
      " cad as a cad\n",
      " Gener aplane sketch for 0wpsch =qWork(qPl(qVector0000125 0006,.,.),qVector100600000001 00 00006;qVector612399367e17 -.,006 -.2335766644-).To006 00074742263,.To00742263474253line1line006line106line007line001line003line000line06line0100101line01line06ext0007ext0\n",
      "\n",
      "=== Ground Truth CadQuery Code ===\n",
      "import cadquery as cq\n",
      "# Generating a workplane for sketch 0\n",
      "wp_sketch0 = cq.Workplane(cq.Plane(cq.Vector(0.0, 0.0, -0.078125), cq.Vector(1.0, 6.123233995736766e-17, -6.123233995736766e-17), cq.Vector(6.123233995736766e-17, -1.0, 6.123233995736766e-17)))\n",
      "loop0=wp_sketch0.moveTo(0.039473684210526314, 0.0).lineTo(0.039473684210526314, 0.05921052631578947).lineTo(0.29605263157894735, 0.05921052631578947).lineTo(0.29605263157894735, 0.0).lineTo(0.3289473684210526, 0.0).lineTo(0.3289473684210526, 0.05921052631578947\n",
      "\n",
      "🔵 BLEU score (sample 0): 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Inference Cell: Evaluate generated code using BLEU score\n",
    "# -------------------------------------------------\n",
    "# Steps:\n",
    "# 1. Load the trained model, tokenizer, and image processor.\n",
    "# 2. Load the processed test dataset.\n",
    "# 3. Define a function to compute the BLEU score for a single prediction.\n",
    "# 4. Run inference on a sample and compute BLEU score compared to the ground truth.\n",
    "# 5. Optionally, compute the average BLEU score over multiple samples.\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoImageProcessor\n",
    "from datasets import load_from_disk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "# Specify paths for checkpoint and processed data\n",
    "CHECKPOINT_DIR = \"baseline_model_checkpoint-20250830T093207Z-1-001/baseline_model_checkpoint\"\n",
    "PROCESSED_DATA_DIR = \"/Volumes/BIG-DATA/processed_gencad\"\n",
    "\n",
    "print(\"Loading trained model...\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(CHECKPOINT_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Move model to GPU if available and set to evaluation mode\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "\n",
    "print(\"Loading test dataset...\")\n",
    "test_ds = load_from_disk(f\"{PROCESSED_DATA_DIR}/test_shard0\")\n",
    "print(f\"Test samples: {len(test_ds)}\")\n",
    "\n",
    "# Function to calculate the BLEU score for a single prediction vs reference text\n",
    "def calculate_bleu(pred_text, ref_text):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    pred_tokens = pred_text.split()\n",
    "    ref_tokens = ref_text.split()\n",
    "    return sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
    "\n",
    "# Run inference and compute BLEU score for one sample\n",
    "sample = test_ds[0]\n",
    "pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        pixel_values=pixel_values,\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "ground_truth_text = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== Predicted CadQuery Code ===\")\n",
    "print(generated_text)\n",
    "print(\"\\n=== Ground Truth CadQuery Code ===\")\n",
    "print(ground_truth_text)\n",
    "\n",
    "# Compute BLEU score for the sample and print it\n",
    "bleu_score = calculate_bleu(generated_text, ground_truth_text)\n",
    "print(f\"\\n BLEU score (sample 0): {bleu_score:.4f}\")\n",
    "\n",
    "# Optionally, compute the average BLEU score over N samples\n",
    "N = 50  # Adjust the number of samples as needed\n",
    "total_bleu = 0\n",
    "for i in range(N):\n",
    "    sample = test_ds[i]\n",
    "    pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values=pixel_values, max_length=128, num_beams=4)\n",
    "    pred = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    ref = tokenizer.decode(sample[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    total_bleu += calculate_bleu(pred, ref)\n",
    "\n",
    "avg_bleu = total_bleu / N\n",
    "print(f\"\\nAverage BLEU score over {N} samples: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3efbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
